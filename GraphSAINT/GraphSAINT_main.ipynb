{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRU1I8TIl614",
        "outputId": "37bc2b33-f288-40e5-8b54-0846d6eb8ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# upload your code to your Google Drive to import the data automatically\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_folder_path = '/content/drive/MyDrive/data/NML_Final_Project/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ySc7xVZamE3B"
      },
      "outputs": [],
      "source": [
        "ROOT = \"/content/drive/MyDrive/data/NML_Final_Project_GraphSAINT/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T1nyvvAKmWte"
      },
      "outputs": [],
      "source": [
        "args = [\"--gpu\", \"-5\", \n",
        "        \"--dataset\" , \"amazon\" , \n",
        "        \"--sampler\", \"node\", \n",
        "        \"--node-budget\", \"4500\",\n",
        "        \"--num-repeat\", \"50\",\n",
        "        \"--n-epochs\", \"30\",\n",
        "        \"--n-hidden\", \"512\",\n",
        "        \"--arch\", \"1-1-0\",\n",
        "        \"--dropout\", \"0.1\"]\n",
        "args_str = ' '.join(args)\n",
        "\n",
        "args_dict = {\"gpu\" : int(args[1]), \n",
        "            \"dataset\" : args[3],\n",
        "            \"sampler\" : args[5],\n",
        "            \"node-budget\" : int(args[7]),\n",
        "            \"num-repeat\" : int(args[9]),\n",
        "            \"n-epochs\" : int(args[11]),\n",
        "            \"n-hidden\" : int(args[13]),\n",
        "            \"arch\" : args[15],\n",
        "            \"dropout\" : float(args[17])\n",
        "            }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NObOh9_pUA5",
        "outputId": "b29cf37a-e4c8-4a73-b500-9554b639acad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgi8EmvImhXP",
        "outputId": "d3d3ad0b-c8e7-482e-8a9f-410978b74e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are currently in the folder:\n",
            "\n",
            "/content/drive/MyDrive/data/NML_Final_Project_GraphSAINT\n",
            "\n",
            "In the mentioned folder these are the files:\n",
            "\n",
            "data\t      modules.py     __pycache__  subgraphs\t     utils.py\n",
            "in_feats.npy  n_classes.npy  README.md\t  train_sampling.py\n",
            "log\t      prob.pt\t     sampler.py   Untitled0.ipynb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(ROOT)\n",
        "print(\"You are currently in the folder:\\n\")\n",
        "!pwd\n",
        "print(\"\\nIn the mentioned folder these are the files:\\n\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "76WMuac5SHGF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from modules import GCNNet\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1DqKac5TTog"
      },
      "source": [
        "***Node Sampler***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvOKGU1n8cC",
        "outputId": "198c4f77-b820-4ff0-a60f-6d9801762486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n",
            "Namespace(aggr='concat', arch='1-1-0', dataset='amazon', dropout=0.1, edge_budget=4000, gpu=-5, length=2, lr=0.01, n_epochs=30, n_hidden=512, no_batch_norm=False, node_budget=4500, note='none', num_repeat=50, num_roots=3000, sampler='node', use_val=False, val_every=1)\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x122d68000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f86271a7 0x7f70f8627449 0x7f70f86274a6 0x7f70f8afb9f8 0x7f70f91e8c0f 0x7f70f8faa656 0x7f70f91ce466 0x7f70f8fe8367 0x7f70f8af8dad 0x7f70f92ea8ea 0x7f70f8db15f9 0x7f70f91cfa47 0x7f70f8db15f9 0x7f70f9e034c6 0x7f70f9e03a0d 0x7f70f8e1ef4a 0x7f70f8af43c9 0x7f70f93812c2 0x7f70f8f11813 0x7f7173fb5866 0x7f7173fbc7c8 0x7f7173cc609f 0x593835 0x548c51 0x5127f1 0x549576 0x593fce\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x1a0e2a000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f86271a7 0x7f70f8627449 0x7f70f86274a6 0x7f70f8afb9f8 0x7f70f91e8c0f 0x7f70f8faa656 0x7f70f91ce466 0x7f70f8fe8367 0x7f70f8af8dad 0x7f70f92ea8ea 0x7f70f8db15f9 0x7f70f91cfa47 0x7f70f8db15f9 0x7f70f9e034c6 0x7f70f9e03a0d 0x7f70f8e1ef4a 0x7f70f8af43c9 0x7f70f93812c2 0x7f70f8f11813 0x7f7173fb5866 0x7f7173fbc7c8 0x7f7173cc609f 0x593835 0x548c51 0x5127f1 0x549576 0x593fce\n",
            "tcmalloc: large alloc 2009554944 bytes == 0x26a4b6000 @  0x7f718d4311e7 0x7f718ad420ce 0x7f718ad98cf5 0x7f718ad98e08 0x7f718ae580f4 0x7f718ae5b30c 0x7f718afe23ac 0x7f718afe2e10 0x59588e 0x595b69 0x7f718ae622a6 0x4d0bb9 0x5124f8 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c\n",
            "----Data statistics------'\n",
            "    #Nodes 1569960\n",
            "    #Edges 264339468\n",
            "    #Classes 107\n",
            "    #Train samples 1255968\n",
            "    #Val samples 78498\n",
            "    #Test samples 235494\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x621ce000 @  0x7f718d413b6b 0x7f718d433379 0x7f70bad7aad1 0x7f70bad94e81 0x7f70ba77312f 0x7f70ba66c940 0x7f70baeaece6 0x7f70baeaf6cd 0x7f70baeb1357 0x7f70badd901b 0x7f70badeb4d6 0x7f70bad78d48 0x7f70b9ad2623 0x7f70b9ad294b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x360c7a000 @  0x7f718d413b6b 0x7f718d433379 0x7f70bad7aad1 0x7f70bad94e81 0x7f70ba773186 0x7f70ba66c940 0x7f70baeaece6 0x7f70baeaf6cd 0x7f70baeb1357 0x7f70badd901b 0x7f70badeb4d6 0x7f70bad78d48 0x7f70b9ad2623 0x7f70b9ad294b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 2147483648 bytes == 0x3ded3c000 @  0x7f718d433887 0x7f70ba6990fe 0x7f70ba785eab 0x7f70ba67b114 0x7f70baebaacf 0x7f70baeb136e 0x7f70badd901b 0x7f70badeb4d6 0x7f70bad78d48 0x7f70b9ad2623 0x7f70b9ad294b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x532c1b 0x594a96 0x515600\n",
            "The number of subgraphs is:  14584\n",
            "The size of subgraphs is about:  4304\n",
            "labels shape: torch.Size([1569960, 107])\n",
            "features shape: torch.Size([1569960, 200])\n",
            "Namespace(aggr='concat', arch='1-1-0', dataset='amazon', dropout=0.1, edge_budget=4000, gpu=-5, length=2, lr=0.01, n_epochs=30, n_hidden=512, no_batch_norm=False, node_budget=4500, note='none', num_repeat=50, num_roots=3000, sampler='node', use_val=False, val_every=1)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:1/30, Iteration 280/280:training loss 3.1154725551605225\n",
            "Finished training epoch = 1. The training execution time = 458.45 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "tcmalloc: large alloc 3215278080 bytes == 0x6c89ce000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f862810f 0x7f70f8628a51 0x7f70f8628aa4 0x7f70f8628bb2 0x7f70f91f0c28 0x7f70f922b7b2 0x7f70f89278d4 0x7f70f91f67d7 0x7f70f91f682f 0x7f70f9075fc7 0x7f70f9076492 0x7f70fa00a5cc 0x7f70fa00b332 0x7f70f90b8b96 0x7f70f8936e92 0x7f70f893768a 0x7f70f937ee7f 0x7f70f91680a6 0x7f70f891a413 0x7f70f937eb52 0x7f70f8e096e5 0x7f7173bfb228 0x593784 0x548c51 0x51566f 0x4bc98a 0x59c019\n",
            "tcmalloc: large alloc 3215278080 bytes == 0x788422000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f862810f 0x7f70f8628a51 0x7f70f8628aa4 0x7f70f8628bb2 0x7f70f91f0c28 0x7f70f922982f 0x7f70f866d68e 0x7f70f866ecbb 0x7f70f8670074 0x7f70f881edaf 0x7f70f920bb43 0x7f70f920bbf2 0x7f70f8f9ab39 0x7f70f9f3ea72 0x7f70f9f3f1c5 0x7f70f8fc6f4a 0x7f7173ae106f 0x7f7173ae1366 0x593a14 0x594cd3 0x531a7c 0x4d1308 0x5122db 0x593dd7 0x548ae9 0x51566f 0x549e0e\n",
            "tcmalloc: large alloc 3215278080 bytes == 0x6c89ce000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f8a712d8 0x7f70f8a71de7 0x7f70f8a72c80 0x7f70f866b180 0x7f70f866a166 0x7f70f866eb0a 0x7f70f866fc15 0x7f70f866fce4 0x7f70f8aee85b 0x7f70f90c3212 0x7f70f8aee2f8 0x7f70f92e96cf 0x7f70f90c097d 0x7f70f87a1b80 0x7f70f91e9adc 0x7f70f9139372 0x7f70f913a62e 0x7f70fa0a8a3d 0x7f70fa0a916f 0x7f70f916db70 0x7f7173bce6c3 0x593784 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 3215278080 bytes == 0x788422000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f862810f 0x7f70f8628a51 0x7f70f8628aa4 0x7f70f8628bb2 0x7f70f91f0c28 0x7f70f922cc6f 0x7f70f866d68e 0x7f70f866ecbb 0x7f70f8670074 0x7f70f881eece 0x7f70f92189b3 0x7f70f9218a62 0x7f70f914f4b9 0x7f70fa0b4ae2 0x7f70fa0b5235 0x7f70f918e40a 0x7f7173ae0bef 0x7f7173ae0ee6 0x593a14 0x594cd3 0x53187a 0x4d0bb9 0x5124f8 0x593dd7 0x548ae9 0x51566f 0x549e0e\n",
            "tcmalloc: large alloc 6430556160 bytes == 0x6c89ce000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f8a712d8 0x7f70f8a71de7 0x7f70f9182928 0x7f70f8b15b07 0x7f70f8b16415 0x7f70f91ebe82 0x7f70f90bea2d 0x7f70f8b212e1 0x7f70f92e9622 0x7f70f8dbcc8c 0x7f70f9e0c5bd 0x7f70f9e0ce15 0x7f70f8dfa79a 0x7f7173baccfe 0x593784 0x548c51 0x51566f 0x549e0e 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600\n",
            "tcmalloc: large alloc 6430556160 bytes == 0x848676000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f862810f 0x7f70f8628a51 0x7f70f8628aa4 0x7f70f8afb9ae 0x7f70f91e8aea 0x7f70f8faa38c 0x7f70f91cf62f 0x7f70f8fe51e0 0x7f7173db4371 0x7f70f8affcf0 0x7f70f9380994 0x7f70f8dcf467 0x7f70f91ce995 0x7f70f8e12a59 0x7f7173b005e1 0x593835 0x548c51 0x5127f1 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x4bc98a 0x5134a6 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 6430556160 bytes == 0x9c7b1e000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f862810f 0x7f70f8628a51 0x7f70f8628aa4 0x7f70f8628bb2 0x7f70f91f0c28 0x7f70f922b82f 0x7f70f866d68e 0x7f70f866ecbb 0x7f70f8670074 0x7f70f921346b 0x7f70f92134ff 0x7f70f8dab137 0x7f70f8dab902 0x7f70f9e31786 0x7f70f9e32052 0x7f70f8df0616 0x7f7173ae655f 0x7f7173ae67a6 0x593a14 0x594cd3 0x5af072 0x61678c 0x4d34e0 0x51286f 0x549e0e 0x4bcb19 0x59c019\n",
            "tcmalloc: large alloc 6430556160 bytes == 0x848676000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f8a712d8 0x7f70f8a71de7 0x7f70f9182928 0x7f70f8b15b07 0x7f70f8b16415 0x7f70f91ebe82 0x7f70f90bea2d 0x7f70f8b212e1 0x7f70f92e9622 0x7f70f8dbcc8c 0x7f70f9e0c5bd 0x7f70f9e0ce15 0x7f70f8dfa79a 0x7f7173baccfe 0x593784 0x548c51 0x51566f 0x549e0e 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600\n",
            "Val F1-mic 0.0006, Val F1-mac 0.0004\n",
            "new best val f1: 0.0006139846818693473\n",
            "Finished evaluating epoch = 1. The evaluating execution time = 258.83 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:2/30, Iteration 280/280:training loss 2.2373924255371094\n",
            "Finished training epoch = 2. The training execution time = 463.03 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "tcmalloc: large alloc 6430556160 bytes == 0x67dc04000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f8a712d8 0x7f70f8a71de7 0x7f70f9182928 0x7f70f8b15b07 0x7f70f8b16415 0x7f70f91ebe82 0x7f70f90bea2d 0x7f70f8b212e1 0x7f70f92e9622 0x7f70f8dbcc8c 0x7f70f9e0c5bd 0x7f70f9e0ce15 0x7f70f8dfa79a 0x7f7173baccfe 0x593784 0x548c51 0x51566f 0x549e0e 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600\n",
            "tcmalloc: large alloc 6430556160 bytes == 0xb477c6000 @  0x7f718d413b6b 0x7f718d433379 0x7f70be6c450e 0x7f70be6b67c2 0x7f70f862810f 0x7f70f8628a51 0x7f70f8628aa4 0x7f70f8afb9ae 0x7f70f91e8aea 0x7f70f8faa38c 0x7f70f91cf62f 0x7f70f8fe51e0 0x7f7173db4371 0x7f70f8affcf0 0x7f70f9380994 0x7f70f8dcf467 0x7f70f91ce995 0x7f70f8e12a59 0x7f7173b005e1 0x593835 0x548c51 0x5127f1 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x4bc98a 0x5134a6 0x549e0e 0x4bcb19\n",
            "Val F1-mic 0.3420, Val F1-mac 0.0165\n",
            "new best val f1: 0.3419674173140591\n",
            "Finished evaluating epoch = 2. The evaluating execution time = 254.0 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:3/30, Iteration 280/280:training loss 1.6251471042633057\n",
            "Finished training epoch = 3. The training execution time = 468.54 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.5357, Val F1-mac 0.0915\n",
            "new best val f1: 0.5357331522149272\n",
            "Finished evaluating epoch = 3. The evaluating execution time = 254.39 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:4/30, Iteration 280/280:training loss 1.7107818126678467\n",
            "Finished training epoch = 4. The training execution time = 468.13 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.5727, Val F1-mac 0.1202\n",
            "new best val f1: 0.5727315347568512\n",
            "Finished evaluating epoch = 4. The evaluating execution time = 250.74 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:5/30, Iteration 280/280:training loss 1.4209518432617188\n",
            "Finished training epoch = 5. The training execution time = 475.7 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.6236, Val F1-mac 0.1746\n",
            "new best val f1: 0.6236225738300428\n",
            "Finished evaluating epoch = 5. The evaluating execution time = 249.31 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:6/30, Iteration 280/280:training loss 1.2137315273284912\n",
            "Finished training epoch = 6. The training execution time = 475.5 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.6688, Val F1-mac 0.2367\n",
            "new best val f1: 0.6687783195134904\n",
            "Finished evaluating epoch = 6. The evaluating execution time = 248.57 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:7/30, Iteration 280/280:training loss 1.2079943418502808\n",
            "Finished training epoch = 7. The training execution time = 476.4 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.6876, Val F1-mac 0.2817\n",
            "new best val f1: 0.6875574551967929\n",
            "Finished evaluating epoch = 7. The evaluating execution time = 247.9 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:8/30, Iteration 280/280:training loss 1.0704915523529053\n",
            "Finished training epoch = 8. The training execution time = 478.02 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7078, Val F1-mac 0.3298\n",
            "new best val f1: 0.7077895454524514\n",
            "Finished evaluating epoch = 8. The evaluating execution time = 245.56 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:9/30, Iteration 280/280:training loss 1.1318782567977905\n",
            "Finished training epoch = 9. The training execution time = 478.97 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7145, Val F1-mac 0.3532\n",
            "new best val f1: 0.7144793097312022\n",
            "Finished evaluating epoch = 9. The evaluating execution time = 244.32 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:10/30, Iteration 280/280:training loss 1.4063979387283325\n",
            "Finished training epoch = 10. The training execution time = 476.45 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7231, Val F1-mac 0.3848\n",
            "new best val f1: 0.7230633610818278\n",
            "Finished evaluating epoch = 10. The evaluating execution time = 242.97 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:11/30, Iteration 280/280:training loss 1.1232404708862305\n",
            "Finished training epoch = 11. The training execution time = 472.79 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7222, Val F1-mac 0.3870\n",
            "Finished evaluating epoch = 11. The evaluating execution time = 243.1 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:12/30, Iteration 280/280:training loss 1.021357774734497\n",
            "Finished training epoch = 12. The training execution time = 477.75 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7326, Val F1-mac 0.4160\n",
            "new best val f1: 0.7326313317860402\n",
            "Finished evaluating epoch = 12. The evaluating execution time = 244.9 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:13/30, Iteration 280/280:training loss 1.2298035621643066\n",
            "Finished training epoch = 13. The training execution time = 478.89 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7386, Val F1-mac 0.4361\n",
            "new best val f1: 0.738578078675652\n",
            "Finished evaluating epoch = 13. The evaluating execution time = 244.67 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:14/30, Iteration 280/280:training loss 1.090223789215088\n",
            "Finished training epoch = 14. The training execution time = 480.69 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7453, Val F1-mac 0.4521\n",
            "new best val f1: 0.7452764046539284\n",
            "Finished evaluating epoch = 14. The evaluating execution time = 243.11 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:15/30, Iteration 280/280:training loss 1.1863359212875366\n",
            "Finished training epoch = 15. The training execution time = 478.63 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7463, Val F1-mac 0.4628\n",
            "new best val f1: 0.7463105925797087\n",
            "Finished evaluating epoch = 15. The evaluating execution time = 243.75 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:16/30, Iteration 280/280:training loss 1.1232911348342896\n",
            "Finished training epoch = 16. The training execution time = 482.7 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7450, Val F1-mac 0.4695\n",
            "Finished evaluating epoch = 16. The evaluating execution time = 247.09 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:17/30, Iteration 280/280:training loss 1.229777216911316\n",
            "Finished training epoch = 17. The training execution time = 472.8 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7553, Val F1-mac 0.4966\n",
            "new best val f1: 0.7552709694573636\n",
            "Finished evaluating epoch = 17. The evaluating execution time = 243.11 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:18/30, Iteration 280/280:training loss 1.1996064186096191\n",
            "Finished training epoch = 18. The training execution time = 482.79 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7484, Val F1-mac 0.4736\n",
            "Finished evaluating epoch = 18. The evaluating execution time = 245.87 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:19/30, Iteration 280/280:training loss 1.0390598773956299\n",
            "Finished training epoch = 19. The training execution time = 471.39 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7572, Val F1-mac 0.4935\n",
            "new best val f1: 0.7572485511658569\n",
            "Finished evaluating epoch = 19. The evaluating execution time = 243.63 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:20/30, Iteration 280/280:training loss 0.9859384894371033\n",
            "Finished training epoch = 20. The training execution time = 476.37 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7611, Val F1-mac 0.5111\n",
            "new best val f1: 0.7610556962461453\n",
            "Finished evaluating epoch = 20. The evaluating execution time = 245.95 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:21/30, Iteration 280/280:training loss 1.1452488899230957\n",
            "Finished training epoch = 21. The training execution time = 476.94 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7626, Val F1-mac 0.5115\n",
            "new best val f1: 0.7625955592663135\n",
            "Finished evaluating epoch = 21. The evaluating execution time = 247.69 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:22/30, Iteration 280/280:training loss 0.9956461191177368\n",
            "Finished training epoch = 22. The training execution time = 479.94 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7608, Val F1-mac 0.5142\n",
            "Finished evaluating epoch = 22. The evaluating execution time = 245.97 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:23/30, Iteration 280/280:training loss 1.0424555540084839\n",
            "Finished training epoch = 23. The training execution time = 477.35 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7615, Val F1-mac 0.5206\n",
            "Finished evaluating epoch = 23. The evaluating execution time = 247.28 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:24/30, Iteration 280/280:training loss 1.0737651586532593\n",
            "Finished training epoch = 24. The training execution time = 481.22 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7617, Val F1-mac 0.5117\n",
            "Finished evaluating epoch = 24. The evaluating execution time = 247.6 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:25/30, Iteration 280/280:training loss 1.0839214324951172\n",
            "Finished training epoch = 25. The training execution time = 479.44 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7669, Val F1-mac 0.5323\n",
            "new best val f1: 0.7668770996519652\n",
            "Finished evaluating epoch = 25. The evaluating execution time = 246.49 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:26/30, Iteration 280/280:training loss 1.022848129272461\n",
            "Finished training epoch = 26. The training execution time = 487.01 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7668, Val F1-mac 0.5371\n",
            "Finished evaluating epoch = 26. The evaluating execution time = 247.48 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:27/30, Iteration 280/280:training loss 0.9222259521484375\n",
            "Finished training epoch = 27. The training execution time = 477.95 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7678, Val F1-mac 0.5343\n",
            "new best val f1: 0.7678049587784663\n",
            "Finished evaluating epoch = 27. The evaluating execution time = 250.97 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:28/30, Iteration 280/280:training loss 0.9804739952087402\n",
            "Finished training epoch = 28. The training execution time = 484.83 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7720, Val F1-mac 0.5536\n",
            "new best val f1: 0.771955763965598\n",
            "Finished evaluating epoch = 28. The evaluating execution time = 245.2 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:29/30, Iteration 280/280:training loss 1.0396459102630615\n",
            "Finished training epoch = 29. The training execution time = 489.23 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7733, Val F1-mac 0.5518\n",
            "new best val f1: 0.7732531017688321\n",
            "Finished evaluating epoch = 29. The evaluating execution time = 245.84 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:30/30, Iteration 280/280:training loss 0.9087072014808655\n",
            "Finished training epoch = 30. The training execution time = 479.7 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7742, Val F1-mac 0.5526\n",
            "new best val f1: 0.7741739219853294\n",
            "Finished evaluating epoch = 30. The evaluating execution time = 246.81 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "training using time 21720.732135772705\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Test F1-mic 0.7757, Test F1-mac 0.5526\n",
            " ---------------------------------------------------------------------------------------------------------------- \n"
          ]
        }
      ],
      "source": [
        "!python train_sampling.py \\\n",
        "--gpu=-5 \\\n",
        "--dataset=amazon \\\n",
        "--sampler=node \\\n",
        "--node-budget=4500 \\\n",
        "--num-repeat=50 \\\n",
        "--n-epochs=30 \\\n",
        "--n-hidden=512 \\\n",
        "--arch=1-1-0 \\\n",
        "--dropout=0.1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vanW8qNpTL2",
        "outputId": "a4fe111f-5a2f-4394-958f-e9f63cde41e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200, 107)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "in_feats = int(np.load('in_feats.npy'))\n",
        "n_classes = int(np.load('n_classes.npy'))\n",
        "\n",
        "in_feats, n_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkeV3jWXp7tn",
        "outputId": "0c7f56d8-e1aa-4efa-f6f9-1c6f584456b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GCNNet(\n",
              "  (gcn): ModuleList(\n",
              "    (0): GCNLayer(\n",
              "      (lins): ModuleList(\n",
              "        (0): Linear(in_features=200, out_features=512, bias=False)\n",
              "        (1): Linear(in_features=200, out_features=512, bias=False)\n",
              "      )\n",
              "      (bias): ParameterList(\n",
              "          (0): Parameter containing: [torch.FloatTensor of size 512]\n",
              "          (1): Parameter containing: [torch.FloatTensor of size 512]\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): GCNLayer(\n",
              "      (lins): ModuleList(\n",
              "        (0): Linear(in_features=1024, out_features=512, bias=False)\n",
              "        (1): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      )\n",
              "      (bias): ParameterList(\n",
              "          (0): Parameter containing: [torch.FloatTensor of size 512]\n",
              "          (1): Parameter containing: [torch.FloatTensor of size 512]\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): GCNLayer(\n",
              "      (lins): ModuleList(\n",
              "        (0): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      )\n",
              "      (bias): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 512])\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (out_layer): GCNLayer(\n",
              "    (lins): ModuleList(\n",
              "      (0): Linear(in_features=512, out_features=107, bias=False)\n",
              "    )\n",
              "    (bias): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 107])\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model_path = './log/amazon/none/best_model.pkl'\n",
        "#best_model_ordered_dict = torch.load(best_model_path)\n",
        "#aggr = \"concat\" #\"mean\"\n",
        "best_model = GCNNet(in_dim = in_feats, \n",
        "                    hid_dim = int(args_dict[\"n-hidden\"]),\n",
        "                    out_dim = n_classes,\n",
        "                    arch = args_dict['arch'],\n",
        "                    dropout = args_dict['dropout'])\n",
        "#model = TheModelClass(*args, **kwargs)\n",
        "best_model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"), strict=False)\n",
        "best_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inSCB2t3TMgg"
      },
      "source": [
        "***Edge Sampler***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KGZnZC9sFD5d",
        "outputId": "8d7368c9-d373-4669-af24-596d8f6258e6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.11.0+cu113'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "HiddRPLrTv-8",
        "outputId": "645384b4-83b6-4882-ccf8-137eb2798c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n",
            "Namespace(aggr='concat', arch='1-1-0', dataset='amazon', dropout=0.1, edge_budget=2000, gpu=-5, length=2, lr=0.01, n_epochs=30, n_hidden=512, no_batch_norm=False, node_budget=6000, note='none', num_repeat=50, num_roots=3000, sampler='edge', use_val=False, val_every=1)\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x121f42000 @  0x7efc60ae2b6b 0x7efc60b02379 0x7efb91d9350e 0x7efb91d857c2 0x7efbcbcf61a7 0x7efbcbcf6449 0x7efbcbcf64a6 0x7efbcc1ca9f8 0x7efbcc8b7c0f 0x7efbcc679656 0x7efbcc89d466 0x7efbcc6b7367 0x7efbcc1c7dad 0x7efbcc9b98ea 0x7efbcc4805f9 0x7efbcc89ea47 0x7efbcc4805f9 0x7efbcd4d24c6 0x7efbcd4d2a0d 0x7efbcc4edf4a 0x7efbcc1c33c9 0x7efbcca502c2 0x7efbcc5e0813 0x7efc47684866 0x7efc4768b7c8 0x7efc4739509f 0x593835 0x548c51 0x5127f1 0x549576 0x593fce\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x1a0004000 @  0x7efc60ae2b6b 0x7efc60b02379 0x7efb91d9350e 0x7efb91d857c2 0x7efbcbcf61a7 0x7efbcbcf6449 0x7efbcbcf64a6 0x7efbcc1ca9f8 0x7efbcc8b7c0f 0x7efbcc679656 0x7efbcc89d466 0x7efbcc6b7367 0x7efbcc1c7dad 0x7efbcc9b98ea 0x7efbcc4805f9 0x7efbcc89ea47 0x7efbcc4805f9 0x7efbcd4d24c6 0x7efbcd4d2a0d 0x7efbcc4edf4a 0x7efbcc1c33c9 0x7efbcca502c2 0x7efbcc5e0813 0x7efc47684866 0x7efc4768b7c8 0x7efc4739509f 0x593835 0x548c51 0x5127f1 0x549576 0x593fce\n",
            "tcmalloc: large alloc 2009554944 bytes == 0x269690000 @  0x7efc60b001e7 0x7efc5e4110ce 0x7efc5e467cf5 0x7efc5e467e08 0x7efc5e5270f4 0x7efc5e52a30c 0x7efc5e6b13ac 0x7efc5e6b1e10 0x59588e 0x595b69 0x7efc5e5312a6 0x4d0bb9 0x5124f8 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c\n",
            "----Data statistics------'\n",
            "    #Nodes 1569960\n",
            "    #Edges 264339468\n",
            "    #Classes 107\n",
            "    #Train samples 1255968\n",
            "    #Val samples 78498\n",
            "    #Test samples 235494\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x613a8000 @  0x7efc60ae2b6b 0x7efc60b02379 0x7efb8e409ad1 0x7efb8e423e81 0x7efb8de0212f 0x7efb8dcfb940 0x7efb8e53dce6 0x7efb8e53e6cd 0x7efb8e540357 0x7efb8e46801b 0x7efb8e47a4d6 0x7efb8e407d48 0x7efb8d1a1623 0x7efb8d1a194b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x35fe54000 @  0x7efc60ae2b6b 0x7efc60b02379 0x7efb8e409ad1 0x7efb8e423e81 0x7efb8de02186 0x7efb8dcfb940 0x7efb8e53dce6 0x7efb8e53e6cd 0x7efb8e540357 0x7efb8e46801b 0x7efb8e47a4d6 0x7efb8e407d48 0x7efb8d1a1623 0x7efb8d1a194b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 2147483648 bytes == 0x3ddf16000 @  0x7efc60b02887 0x7efb8dd280fe 0x7efb8de14eab 0x7efb8dd0a114 0x7efb8e549acf 0x7efb8e54036e 0x7efb8e46801b 0x7efb8e47a4d6 0x7efb8e407d48 0x7efb8d1a1623 0x7efb8d1a194b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x532c1b 0x594a96 0x515600\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-489918fceb1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python train_sampling.py --gpu=-5 --dataset=amazon --sampler=edge --edge-budget=2000 --num-repeat=50 --n-epochs=30 --n-hidden=512 --arch=1-1-0 --dropout=0.1 '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   result = _run_command(\n\u001b[0;32m--> 447\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    448\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0;31m# read call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_pty_still_connected\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0mcommand_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mShellResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    613\u001b[0m                 )\n\u001b[1;32m    614\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!python train_sampling.py \\\n",
        "--gpu=-5 \\\n",
        "--dataset=amazon \\\n",
        "--sampler=edge \\\n",
        "--edge-budget=2000 \\\n",
        "--num-repeat=50 \\\n",
        "--n-epochs=30 \\\n",
        "--n-hidden=512 \\\n",
        "--arch=1-1-0 \\\n",
        "--dropout=0.1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZomrmE_1WdtZ"
      },
      "source": [
        "***RW Sampler***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukFnYFtMPAeg",
        "outputId": "668fd1ed-f535-4bfa-9de3-b6b5a121d526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n",
            "Namespace(aggr='concat', arch='1-1-0', dataset='amazon', dropout=0.1, edge_budget=4000, gpu=-5, length=2, lr=0.01, n_epochs=30, n_hidden=512, no_batch_norm=False, node_budget=6000, note='none', num_repeat=50, num_roots=1500, sampler='rw', use_val=False, val_every=1)\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x122b24000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064b1a7 0x7f840064b449 0x7f840064b4a6 0x7f8400b1f9f8 0x7f840120cc0f 0x7f8400fce656 0x7f84011f2466 0x7f840100c367 0x7f8400b1cdad 0x7f840130e8ea 0x7f8400dd55f9 0x7f84011f3a47 0x7f8400dd55f9 0x7f8401e274c6 0x7f8401e27a0d 0x7f8400e42f4a 0x7f8400b183c9 0x7f84013a52c2 0x7f8400f35813 0x7f847bfd9866 0x7f847bfe07c8 0x7f847bcea09f 0x593835 0x548c51 0x5127f1 0x549576 0x593fce\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x1a0be6000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064b1a7 0x7f840064b449 0x7f840064b4a6 0x7f8400b1f9f8 0x7f840120cc0f 0x7f8400fce656 0x7f84011f2466 0x7f840100c367 0x7f8400b1cdad 0x7f840130e8ea 0x7f8400dd55f9 0x7f84011f3a47 0x7f8400dd55f9 0x7f8401e274c6 0x7f8401e27a0d 0x7f8400e42f4a 0x7f8400b183c9 0x7f84013a52c2 0x7f8400f35813 0x7f847bfd9866 0x7f847bfe07c8 0x7f847bcea09f 0x593835 0x548c51 0x5127f1 0x549576 0x593fce\n",
            "tcmalloc: large alloc 2009554944 bytes == 0x26a272000 @  0x7f84954551e7 0x7f8492d660ce 0x7f8492dbccf5 0x7f8492dbce08 0x7f8492e7c0f4 0x7f8492e7f30c 0x7f84930063ac 0x7f8493006e10 0x59588e 0x595b69 0x7f8492e862a6 0x4d0bb9 0x5124f8 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c\n",
            "----Data statistics------'\n",
            "    #Nodes 1569960\n",
            "    #Edges 264339468\n",
            "    #Classes 107\n",
            "    #Train samples 1255968\n",
            "    #Val samples 78498\n",
            "    #Test samples 235494\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x61f8a000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c2d5ead1 0x7f83c2d78e81 0x7f83c275712f 0x7f83c2650940 0x7f83c2e92ce6 0x7f83c2e936cd 0x7f83c2e95357 0x7f83c2dbd01b 0x7f83c2dcf4d6 0x7f83c2d5cd48 0x7f83c1af6623 0x7f83c1af694b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 2114723840 bytes == 0x360a36000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c2d5ead1 0x7f83c2d78e81 0x7f83c2757186 0x7f83c2650940 0x7f83c2e92ce6 0x7f83c2e936cd 0x7f83c2e95357 0x7f83c2dbd01b 0x7f83c2dcf4d6 0x7f83c2d5cd48 0x7f83c1af6623 0x7f83c1af694b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 2147483648 bytes == 0x3deaf8000 @  0x7f8495457887 0x7f83c267d0fe 0x7f83c2769eab 0x7f83c265f114 0x7f83c2e9eacf 0x7f83c2e9536e 0x7f83c2dbd01b 0x7f83c2dcf4d6 0x7f83c2d5cd48 0x7f83c1af6623 0x7f83c1af694b 0x594b72 0x515600 0x593dd7 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x5134a6 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x4bcb19 0x532c1b 0x594a96 0x515600\n",
            "Sampling time: [25.38s]\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n",
            "Normalization time: [566.58s]\n",
            "The number of subgraphs is:  19992\n",
            "The size of subgraphs is about:  3105\n",
            "labels shape: torch.Size([1569960, 107])\n",
            "features shape: torch.Size([1569960, 200])\n",
            "Namespace(aggr='concat', arch='1-1-0', dataset='amazon', dropout=0.1, edge_budget=4000, gpu=-5, length=2, lr=0.01, n_epochs=30, n_hidden=512, no_batch_norm=False, node_budget=6000, note='none', num_repeat=50, num_roots=1500, sampler='rw', use_val=False, val_every=1)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:1/30, Iteration 419/419:training loss 4.799976348876953\n",
            "Finished training epoch = 1. The training execution time = 205.24 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "tcmalloc: large alloc 3215278080 bytes == 0x7b5298000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064c10f 0x7f840064ca51 0x7f840064caa4 0x7f840064cbb2 0x7f8401214c28 0x7f840124f7b2 0x7f840094b8d4 0x7f840121a7d7 0x7f840121a82f 0x7f8401099fc7 0x7f840109a492 0x7f840202e5cc 0x7f840202f332 0x7f84010dcb96 0x7f840095ae92 0x7f840095b68a 0x7f84013a2e7f 0x7f840118c0a6 0x7f840093e413 0x7f84013a2b52 0x7f8400e2d6e5 0x7f847bc1f228 0x593784 0x548c51 0x51566f 0x4bc98a 0x59c019\n",
            "tcmalloc: large alloc 3215278080 bytes == 0x8754ec000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064c10f 0x7f840064ca51 0x7f840064caa4 0x7f840064cbb2 0x7f8401214c28 0x7f840124d82f 0x7f840069168e 0x7f8400692cbb 0x7f8400694074 0x7f8400842daf 0x7f840122fb43 0x7f840122fbf2 0x7f8400fbeb39 0x7f8401f62a72 0x7f8401f631c5 0x7f8400feaf4a 0x7f847bb0506f 0x7f847bb05366 0x593a14 0x594cd3 0x531a7c 0x4d1308 0x5122db 0x593dd7 0x548ae9 0x51566f 0x549e0e\n",
            "tcmalloc: large alloc 3215278080 bytes == 0x7b5298000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f8400a952d8 0x7f8400a95de7 0x7f8400a96c80 0x7f840068f180 0x7f840068e166 0x7f8400692b0a 0x7f8400693c15 0x7f8400693ce4 0x7f8400b1285b 0x7f84010e7212 0x7f8400b122f8 0x7f840130d6cf 0x7f84010e497d 0x7f84007c5b80 0x7f840120dadc 0x7f840115d372 0x7f840115e62e 0x7f84020cca3d 0x7f84020cd16f 0x7f8401191b70 0x7f847bbf26c3 0x593784 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 3215278080 bytes == 0x8754ec000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064c10f 0x7f840064ca51 0x7f840064caa4 0x7f840064cbb2 0x7f8401214c28 0x7f8401250c6f 0x7f840069168e 0x7f8400692cbb 0x7f8400694074 0x7f8400842ece 0x7f840123c9b3 0x7f840123ca62 0x7f84011734b9 0x7f84020d8ae2 0x7f84020d9235 0x7f84011b240a 0x7f847bb04bef 0x7f847bb04ee6 0x593a14 0x594cd3 0x53187a 0x4d0bb9 0x5124f8 0x593dd7 0x548ae9 0x51566f 0x549e0e\n",
            "tcmalloc: large alloc 6430556160 bytes == 0xab4be8000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f8400a952d8 0x7f8400a95de7 0x7f84011a6928 0x7f8400b39b07 0x7f8400b3a415 0x7f840120fe82 0x7f84010e2a2d 0x7f8400b452e1 0x7f840130d622 0x7f8400de0c8c 0x7f8401e305bd 0x7f8401e30e15 0x7f8400e1e79a 0x7f847bbd0cfe 0x593784 0x548c51 0x51566f 0x549e0e 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600\n",
            "tcmalloc: large alloc 6430556160 bytes == 0x8754ec000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064c10f 0x7f840064ca51 0x7f840064caa4 0x7f8400b1f9ae 0x7f840120caea 0x7f8400fce38c 0x7f84011f362f 0x7f84010091e0 0x7f847bdd8371 0x7f8400b23cf0 0x7f84013a4994 0x7f8400df3467 0x7f84011f2995 0x7f8400e36a59 0x7f847bb245e1 0x593835 0x548c51 0x5127f1 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x4bc98a 0x5134a6 0x549e0e 0x4bcb19\n",
            "tcmalloc: large alloc 6430556160 bytes == 0xc34890000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064c10f 0x7f840064ca51 0x7f840064caa4 0x7f840064cbb2 0x7f8401214c28 0x7f840124f82f 0x7f840069168e 0x7f8400692cbb 0x7f8400694074 0x7f840123746b 0x7f84012374ff 0x7f8400dcf137 0x7f8400dcf902 0x7f8401e55786 0x7f8401e56052 0x7f8400e14616 0x7f847bb0a55f 0x7f847bb0a7a6 0x593a14 0x594cd3 0x5af072 0x61678c 0x4d34e0 0x51286f 0x549e0e 0x4bcb19 0x59c019\n",
            "tcmalloc: large alloc 6430556160 bytes == 0xdb3d38000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f8400a952d8 0x7f8400a95de7 0x7f84011a6928 0x7f8400b39b07 0x7f8400b3a415 0x7f840120fe82 0x7f84010e2a2d 0x7f8400b452e1 0x7f840130d622 0x7f8400de0c8c 0x7f8401e305bd 0x7f8401e30e15 0x7f8400e1e79a 0x7f847bbd0cfe 0x593784 0x548c51 0x51566f 0x549e0e 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600\n",
            "Val F1-mic 0.4263, Val F1-mac 0.0315\n",
            "new best val f1: 0.42626102829327656\n",
            "Finished evaluating epoch = 1. The evaluating execution time = 264.92 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:2/30, Iteration 419/419:training loss 2.731551170349121\n",
            "Finished training epoch = 2. The training execution time = 191.2 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "tcmalloc: large alloc 6430556160 bytes == 0x934f40000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f8400a952d8 0x7f8400a95de7 0x7f84011a6928 0x7f8400b39b07 0x7f8400b3a415 0x7f840120fe82 0x7f84010e2a2d 0x7f8400b452e1 0x7f840130d622 0x7f8400de0c8c 0x7f8401e305bd 0x7f8401e30e15 0x7f8400e1e79a 0x7f847bbd0cfe 0x593784 0x548c51 0x51566f 0x549e0e 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600\n",
            "tcmalloc: large alloc 6430556160 bytes == 0xab4be8000 @  0x7f8495437b6b 0x7f8495457379 0x7f83c66e850e 0x7f83c66da7c2 0x7f840064c10f 0x7f840064ca51 0x7f840064caa4 0x7f8400b1f9ae 0x7f840120caea 0x7f8400fce38c 0x7f84011f362f 0x7f84010091e0 0x7f847bdd8371 0x7f8400b23cf0 0x7f84013a4994 0x7f8400df3467 0x7f84011f2995 0x7f8400e36a59 0x7f847bb245e1 0x593835 0x548c51 0x5127f1 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x4bc98a 0x5134a6 0x549e0e 0x4bcb19\n",
            "Val F1-mic 0.6927, Val F1-mac 0.2532\n",
            "new best val f1: 0.6927393896878289\n",
            "Finished evaluating epoch = 2. The evaluating execution time = 265.58 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:3/30, Iteration 419/419:training loss 2.3778045177459717\n",
            "Finished training epoch = 3. The training execution time = 193.71 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7385, Val F1-mac 0.3740\n",
            "new best val f1: 0.7385400363679131\n",
            "Finished evaluating epoch = 3. The evaluating execution time = 261.83 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:4/30, Iteration 419/419:training loss 2.271641969680786\n",
            "Finished training epoch = 4. The training execution time = 204.86 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7621, Val F1-mac 0.4765\n",
            "new best val f1: 0.7620999124963174\n",
            "Finished evaluating epoch = 4. The evaluating execution time = 262.56 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:5/30, Iteration 419/419:training loss 2.156838893890381\n",
            "Finished training epoch = 5. The training execution time = 193.77 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7752, Val F1-mac 0.5225\n",
            "new best val f1: 0.7752178924330134\n",
            "Finished evaluating epoch = 5. The evaluating execution time = 269.09 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:6/30, Iteration 419/419:training loss 2.0501012802124023\n",
            "Finished training epoch = 6. The training execution time = 193.11 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7803, Val F1-mac 0.5423\n",
            "new best val f1: 0.7803021795281576\n",
            "Finished evaluating epoch = 6. The evaluating execution time = 261.37 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:7/30, Iteration 419/419:training loss 2.097552537918091\n",
            "Finished training epoch = 7. The training execution time = 204.93 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7856, Val F1-mac 0.5657\n",
            "new best val f1: 0.7856365241691454\n",
            "Finished evaluating epoch = 7. The evaluating execution time = 264.96 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:8/30, Iteration 419/419:training loss 2.118929147720337\n",
            "Finished training epoch = 8. The training execution time = 207.41 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7912, Val F1-mac 0.5853\n",
            "new best val f1: 0.7912078614849002\n",
            "Finished evaluating epoch = 8. The evaluating execution time = 299.73 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:9/30, Iteration 419/419:training loss 1.9417403936386108\n",
            "Finished training epoch = 9. The training execution time = 193.06 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7928, Val F1-mac 0.5869\n",
            "new best val f1: 0.7928208896559974\n",
            "Finished evaluating epoch = 9. The evaluating execution time = 264.66 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:10/30, Iteration 419/419:training loss 2.0040736198425293\n",
            "Finished training epoch = 10. The training execution time = 203.53 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7960, Val F1-mac 0.6007\n",
            "new best val f1: 0.7959798909628221\n",
            "Finished evaluating epoch = 10. The evaluating execution time = 266.87 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:11/30, Iteration 419/419:training loss 1.8801891803741455\n",
            "Finished training epoch = 11. The training execution time = 193.97 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.7983, Val F1-mac 0.6082\n",
            "new best val f1: 0.7982790172816757\n",
            "Finished evaluating epoch = 11. The evaluating execution time = 273.27 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:12/30, Iteration 419/419:training loss 1.928478479385376\n",
            "Finished training epoch = 12. The training execution time = 197.28 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8012, Val F1-mac 0.6162\n",
            "new best val f1: 0.8011978961320033\n",
            "Finished evaluating epoch = 12. The evaluating execution time = 261.92 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:13/30, Iteration 419/419:training loss 1.716722846031189\n",
            "Finished training epoch = 13. The training execution time = 205.75 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8014, Val F1-mac 0.6197\n",
            "new best val f1: 0.8014138141423492\n",
            "Finished evaluating epoch = 13. The evaluating execution time = 269.85 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:14/30, Iteration 419/419:training loss 1.8521416187286377\n",
            "Finished training epoch = 14. The training execution time = 206.43 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8018, Val F1-mac 0.6184\n",
            "new best val f1: 0.8017946085700164\n",
            "Finished evaluating epoch = 14. The evaluating execution time = 269.35 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:15/30, Iteration 419/419:training loss 1.8113545179367065\n",
            "Finished training epoch = 15. The training execution time = 194.64 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8044, Val F1-mac 0.6230\n",
            "new best val f1: 0.8043978995860652\n",
            "Finished evaluating epoch = 15. The evaluating execution time = 273.08 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:16/30, Iteration 419/419:training loss 1.7413208484649658\n",
            "Finished training epoch = 16. The training execution time = 195.98 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8054, Val F1-mac 0.6326\n",
            "new best val f1: 0.8054109342119604\n",
            "Finished evaluating epoch = 16. The evaluating execution time = 271.34 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:17/30, Iteration 419/419:training loss 1.733848214149475\n",
            "Finished training epoch = 17. The training execution time = 207.47 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8055, Val F1-mac 0.6262\n",
            "new best val f1: 0.8055216325705575\n",
            "Finished evaluating epoch = 17. The evaluating execution time = 266.29 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:18/30, Iteration 419/419:training loss 1.7397078275680542\n",
            "Finished training epoch = 18. The training execution time = 208.72 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8059, Val F1-mac 0.6276\n",
            "new best val f1: 0.8059058958060069\n",
            "Finished evaluating epoch = 18. The evaluating execution time = 267.26 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:19/30, Iteration 419/419:training loss 1.8295588493347168\n",
            "Finished training epoch = 19. The training execution time = 196.13 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8075, Val F1-mac 0.6355\n",
            "new best val f1: 0.8075365262976751\n",
            "Finished evaluating epoch = 19. The evaluating execution time = 285.68 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:20/30, Iteration 419/419:training loss 1.7620983123779297\n",
            "Finished training epoch = 20. The training execution time = 196.96 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8081, Val F1-mac 0.6389\n",
            "new best val f1: 0.8080717751135589\n",
            "Finished evaluating epoch = 20. The evaluating execution time = 265.17 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:21/30, Iteration 419/419:training loss 1.8029959201812744\n",
            "Finished training epoch = 21. The training execution time = 206.22 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8082, Val F1-mac 0.6403\n",
            "new best val f1: 0.8081978336809832\n",
            "Finished evaluating epoch = 21. The evaluating execution time = 269.44 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:22/30, Iteration 419/419:training loss 1.7683738470077515\n",
            "Finished training epoch = 22. The training execution time = 208.7 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8089, Val F1-mac 0.6401\n",
            "new best val f1: 0.8088820455315712\n",
            "Finished evaluating epoch = 22. The evaluating execution time = 268.64 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:23/30, Iteration 419/419:training loss 1.7974141836166382\n",
            "Finished training epoch = 23. The training execution time = 194.11 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8101, Val F1-mac 0.6418\n",
            "new best val f1: 0.8101372351328606\n",
            "Finished evaluating epoch = 23. The evaluating execution time = 292.93 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:24/30, Iteration 419/419:training loss 1.6329395771026611\n",
            "Finished training epoch = 24. The training execution time = 195.61 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8106, Val F1-mac 0.6452\n",
            "new best val f1: 0.8106307292951165\n",
            "Finished evaluating epoch = 24. The evaluating execution time = 265.06 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:25/30, Iteration 419/419:training loss 1.7649474143981934\n",
            "Finished training epoch = 25. The training execution time = 204.77 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8097, Val F1-mac 0.6421\n",
            "Finished evaluating epoch = 25. The evaluating execution time = 266.36 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:26/30, Iteration 419/419:training loss 1.6202327013015747\n",
            "Finished training epoch = 26. The training execution time = 213.55 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8105, Val F1-mac 0.6433\n",
            "Finished evaluating epoch = 26. The evaluating execution time = 282.6 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:27/30, Iteration 419/419:training loss 1.7375941276550293\n",
            "Finished training epoch = 27. The training execution time = 194.03 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8110, Val F1-mac 0.6435\n",
            "new best val f1: 0.8109902977584477\n",
            "Finished evaluating epoch = 27. The evaluating execution time = 272.23 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:28/30, Iteration 419/419:training loss 1.5220870971679688\n",
            "Finished training epoch = 28. The training execution time = 196.95 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8116, Val F1-mac 0.6467\n",
            "new best val f1: 0.8115595931931665\n",
            "Finished evaluating epoch = 28. The evaluating execution time = 265.98 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:29/30, Iteration 419/419:training loss 1.6682610511779785\n",
            "Finished training epoch = 29. The training execution time = 206.8 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8116, Val F1-mac 0.6435\n",
            "Finished evaluating epoch = 29. The evaluating execution time = 263.82 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "train_sampling.py:121: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
            "epoch:30/30, Iteration 419/419:training loss 1.6829849481582642\n",
            "Finished training epoch = 30. The training execution time = 232.51 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Val F1-mic 0.8127, Val F1-mac 0.6494\n",
            "new best val f1: 0.8127041455326197\n",
            "Finished evaluating epoch = 30. The evaluating execution time = 271.52 sec.\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "training using time 14150.793138980865\n",
            " ---------------------------------------------------------------------------------------------------------------- \n",
            "Test F1-mic 0.8143, Test F1-mac 0.6508\n",
            " ---------------------------------------------------------------------------------------------------------------- \n"
          ]
        }
      ],
      "source": [
        "!python train_sampling.py \\\n",
        "--gpu=-5 \\\n",
        "--dataset=amazon \\\n",
        "--sampler=rw \\\n",
        "--num-roots=1500 \\\n",
        "--length=2 \\\n",
        "--num-repeat=50 \\\n",
        "--n-epochs=30 \\\n",
        "--n-hidden=512 \\\n",
        "--arch=1-1-0 \\\n",
        "--dropout=0.1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DeRvhZKFamOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15efb252-bbdc-45ba-d46e-df5cf4bc00d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNNet(\n",
              "  (gcn): ModuleList(\n",
              "    (0): GCNLayer(\n",
              "      (lins): ModuleList(\n",
              "        (0): Linear(in_features=200, out_features=512, bias=False)\n",
              "        (1): Linear(in_features=200, out_features=512, bias=False)\n",
              "      )\n",
              "      (bias): ParameterList(\n",
              "          (0): Parameter containing: [torch.FloatTensor of size 512]\n",
              "          (1): Parameter containing: [torch.FloatTensor of size 512]\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): GCNLayer(\n",
              "      (lins): ModuleList(\n",
              "        (0): Linear(in_features=1024, out_features=512, bias=False)\n",
              "        (1): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      )\n",
              "      (bias): ParameterList(\n",
              "          (0): Parameter containing: [torch.FloatTensor of size 512]\n",
              "          (1): Parameter containing: [torch.FloatTensor of size 512]\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): GCNLayer(\n",
              "      (lins): ModuleList(\n",
              "        (0): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      )\n",
              "      (bias): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 512])\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (out_layer): GCNLayer(\n",
              "    (lins): ModuleList(\n",
              "      (0): Linear(in_features=512, out_features=107, bias=False)\n",
              "    )\n",
              "    (bias): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 107])\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "in_feats = int(np.load('in_feats.npy'))\n",
        "n_classes = int(np.load('n_classes.npy'))\n",
        "\n",
        "in_feats, n_classes\n",
        "best_model_path = './log/amazon/none/best_model.pkl'\n",
        "#best_model_ordered_dict = torch.load(best_model_path)\n",
        "#aggr = \"concat\" #\"mean\"\n",
        "best_model = GCNNet(in_dim = in_feats, \n",
        "                    hid_dim = int(args_dict[\"n-hidden\"]),\n",
        "                    out_dim = n_classes,\n",
        "                    arch = args_dict['arch'],\n",
        "                    dropout = args_dict['dropout'])\n",
        "#model = TheModelClass(*args, **kwargs)\n",
        "best_model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"), strict=False)\n",
        "best_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NuGoFEo8Zb6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}