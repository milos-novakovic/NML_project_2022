{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_exploitation_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Assignment:  Amazon products dataset"
      ],
      "metadata": {
        "id": "So9t3SVs7z0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Students\n",
        "\n",
        "* Team: `9`\n",
        "* Students: `Mateja Ilić & Miloš Novaković` (for the team submission)"
      ],
      "metadata": {
        "id": "NjwwHBr-JblQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.1. Establishing a baseline\n",
        "\n",
        "In order to see how our Graph-based learning algorithms compare to conventional Machine Learning models, we establish a baseline performance based on a simple Multi-Layer-Perceptron (MLP) architecture. To keep the comparison fair, we shall set the number of hidden layers and hidden dimensions to the same values in all Neural Network-like models, if possible."
      ],
      "metadata": {
        "id": "rNmlBaR0iTH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNq-dcdj7sZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db3bd02-65d1-4365-bec4-46f615279bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 11.8 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 32.1 MB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 17.3 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=c38c775eaf6d5fda939e4fc08864583d701dd8f46ef77f577c1ef026946ff404\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0 torch-geometric-2.0.4 torch-scatter-2.0.9 torch-sparse-0.6.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.0-py3-none-any.whl (418 kB)\n",
            "\u001b[K     |████████████████████████████████| 418 kB 12.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# downloading the necessary packages\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the necessary packages and classes\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch_geometric as pyg\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import AmazonProducts\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "F8OD8PS08_9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-cJyFlRFFyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bfe116-5f3e-4e5d-a396-bea3208c4a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The device used is cuda.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU :(\")\n",
        "    device = 'cpu'\n",
        "\n",
        "print(f\"The device used is {device}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload your code to your Google Drive to import the data automatically\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_folder_path = '/content/drive/MyDrive/data/NML_Final_Project/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo0fa-MYMDSK",
        "outputId": "2ccee873-0676-4f83-dbb9-2356337c3605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we load the dataset\n",
        "dataset = AmazonProducts(root=data_folder_path)\n",
        "data = dataset[0]\n",
        "# in this case, we delete the graph data because it is not needed\n",
        "del data.edge_index\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHO-cCKrmCod",
        "outputId": "c224a3d8-3932-4a2d-e982-a323bf62ba6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[1569960, 200], y=[1569960, 107], train_mask=[1569960], val_mask=[1569960], test_mask=[1569960])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes = data.x.shape[0]\n",
        "num_features = data.x.shape[1]\n",
        "num_classes = data.y.shape[1]\n",
        "num_edges = data.num_edges\n",
        "num_edge_features = data.num_edge_features\n",
        "\n",
        "print(f\"Feature X matrix \\n {data.x}\", end = \"\\n\\n\")\n",
        "print(f\"Feature X matrix shape \\n {(data.x.shape[0], data.x.shape[1])}\", end = \"\\n------------------------------------------------------------------\\n\")\n",
        "print(f\"Label Y matrix \\n {data.y}\", end = \"\\n\\n\")\n",
        "print(f\"Label Y matrix shape \\n {(data.y.shape[0], data.y.shape[1])}\", end = \"\\n------------------------------------------------------------------\\n\")\n",
        "print(f\"Number of nodes N = {num_nodes}\", end = \"\\n\\n\")\n",
        "print(f\"Number of node features D = {num_features}\", end = \"\\n\\n\")\n",
        "print(f\"Number of label classes C = {num_classes}\", end = \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1dPspy9SRFu",
        "outputId": "44e60f5b-dcb1-4037-a2ec-0215424fa67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature X matrix \n",
            " tensor([[-0.1466,  0.2226, -0.3597,  ...,  0.1699,  0.8974,  1.6527],\n",
            "        [-0.2805,  0.0190,  0.4301,  ..., -1.1758, -1.8365, -1.1693],\n",
            "        [ 0.2554,  0.2519, -0.0291,  ...,  1.3751, -0.0735,  0.6262],\n",
            "        ...,\n",
            "        [-0.8121,  0.3626, -0.7781,  ...,  0.0639,  0.8645,  0.0389],\n",
            "        [ 1.5977, -2.3989, -0.0569,  ..., -1.4413,  0.2966,  0.0985],\n",
            "        [-0.1663,  0.0629, -0.0474,  ...,  0.1853, -0.1216, -0.9181]])\n",
            "\n",
            "Feature X matrix shape \n",
            " (1569960, 200)\n",
            "------------------------------------------------------------------\n",
            "Label Y matrix \n",
            " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "\n",
            "Label Y matrix shape \n",
            " (1569960, 107)\n",
            "------------------------------------------------------------------\n",
            "Number of nodes N = 1569960\n",
            "\n",
            "Number of node features D = 200\n",
            "\n",
            "Number of label classes C = 107\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes on the architecture and training:\n",
        "\n",
        "If we want to label our nodes according to which of the 107 categories they belong to, what we get is a multi-class, multi-label, classification problem. For such problems, it is recommended to set the activation function of the output layer as the sigmoid function, which will return values in the (0,1) range. These values can then be interpreted as probabilities and compared to the known outputs in the training data using the binary cross-entropy loss funtction."
      ],
      "metadata": {
        "id": "OaC18jJajPyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we define our MLP architecture with 1 hidden layer\n",
        "hidden_dim1 = 512\n",
        "hidden_dim2 = 512\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.W1 = torch.nn.Linear(num_features, hidden_dim1)\n",
        "        self.W2 = torch.nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.W3 = torch.nn.Linear(hidden_dim2, num_classes)\n",
        "        if dropout:\n",
        "            self.dropout1 = torch.nn.Dropout(p=0.1)\n",
        "            self.dropout2 = torch.nn.Dropout(p=0.5)\n",
        "            self.dropout3 = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.dropout:\n",
        "            x = self.dropout1(F.relu(self.W1(x)))\n",
        "            x = self.dropout2(F.relu(self.W2(x)))\n",
        "            x = self.dropout4(self.W3(x))\n",
        "        else:\n",
        "            x = F.relu(self.W1(x))\n",
        "            x = F.relu(self.W2(x))\n",
        "            x = self.W3(x)\n",
        "        return torch.sigmoid(x) "
      ],
      "metadata": {
        "id": "N3A3THn8VFXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    data: Data,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    nb_epochs: int):\n",
        "    \n",
        "    # puts the model on the device and in the training state\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # container to save the losses for plotting\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # forward pass of the model generates the prediction\n",
        "      y_pred = model(data.x[data.train_mask].to(device))\n",
        "      \n",
        "      # calcualting the loss\n",
        "      loss = F.binary_cross_entropy(input = y_pred,\n",
        "                                    target = data.y[data.train_mask].to(device, dtype=torch.float))\n",
        "      \n",
        "      # after each calculation of loss function, save it to losses\n",
        "      losses.append(loss.item())\n",
        "      \n",
        "      # backward pass (updating the gradients of the parameters of our model)\n",
        "      loss.backward()\n",
        "      \n",
        "      # update of the parameters according to the chosen optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    metric: torchmetrics.Metric,\n",
        "    data: Data,\n",
        "    mask: torch.Tensor):\n",
        "  \n",
        "    model.eval()  # Deactivate dropout\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # do a forward pass (i.e. a prediction) of the model on the features\n",
        "        y_pred = model(data.x.to(device))\n",
        "\n",
        "        # cast the float class prediction probabilites to discrete integer predictions\n",
        "        y_pred = y_pred[mask]\n",
        "\n",
        "        # get the ground truth labels\n",
        "        y = data.y[mask].to(device)\n",
        "\n",
        "        # update the petric that evaluates \n",
        "        metric.update(y_pred, y)\n",
        "        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "    return metric.compute().item()"
      ],
      "metadata": {
        "id": "9HNtLaqpPiCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define whether we use dropout during training or not\n",
        "dropout = False\n",
        "\n",
        "# Define the Model that will be trained\n",
        "model = MLP(dropout)\n",
        "\n",
        "# define the the learning rate\n",
        "lr = 1 * 1e-2\n",
        "\n",
        "# define the optimization algorithm used in the train of the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# define the number of epochs in the training loop\n",
        "nb_epochs = 700#50#100\n",
        "\n",
        "# call the training function\n",
        "start_training_time = time.time()\n",
        "losses_ = train(model, data, optimizer, nb_epochs)\n",
        "end_training_time = time.time()\n",
        "\n",
        "# total training time\n",
        "print(f\"Total training time is {(end_training_time - start_training_time) // 60 : .2f} minutes and {(end_training_time - start_training_time) % 60 : .2f} seconds.\")"
      ],
      "metadata": {
        "id": "ZZIjWRQ6SIx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8545ee-693b-41fb-ae63-a26b80f8854b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training time is  15.00 minutes and  21.72 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cast losses to the numpy array from the regular list\n",
        "losses_ = np.array(losses_)\n",
        "\n",
        "# plot both losses and cummulative mean of the losses\n",
        "plt.plot(np.log(losses_))\n",
        "plt.title(\"Log Loss over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Log Loss\")\n",
        "#plt.xlim([nb_epochs//2, nb_epochs])"
      ],
      "metadata": {
        "id": "zvn14_Dr3Bw-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "bdb21758-ed70-48f8-889a-9bcc3392fc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Log Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes on the testing:\n",
        "\n",
        "In order to test our results properly, we need to set the option `subset_accuracy=True`. That way, the torchmetrics Accuracy module will only count the output of our neural network as correct if all of the labels were assigned correctly, after thresholding the sigmoid outputs of our MLP. Otherwise, we would obtain a deceptively large accuracy, since the labeled outputs are inbalanced, with most of them being 0."
      ],
      "metadata": {
        "id": "iLusZYrjlXd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = torchmetrics.Accuracy(num_classes=num_classes,\n",
        "                               threshold=0.5,\n",
        "                               subset_accuracy=True).to(device)\n",
        "\n",
        "accuracy_train = evaluate(model = model,\n",
        "                          metric = metric,\n",
        "                          data = data,\n",
        "                          mask = data.train_mask)\n",
        "\n",
        "accuracy_val = evaluate(model = model,\n",
        "                        metric = metric,\n",
        "                        data = data,\n",
        "                        mask = data.val_mask)\n",
        "\n",
        "accuracy_test = evaluate(model = model,\n",
        "                        metric = metric,\n",
        "                        data = data,\n",
        "                        mask = data.test_mask)\n",
        "\n",
        "print(\"Train accuracy:\", accuracy_train)\n",
        "print(\"Val accuracy:\", accuracy_val)\n",
        "print(\"Test accuracy:\", accuracy_test)"
      ],
      "metadata": {
        "id": "RejJkjPk6JUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be39d5d0-7e0a-430b-ccc7-b03d1f304694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.6426309943199158\n",
            "Val accuracy: 0.6415952444076538\n",
            "Test accuracy: 0.6393341422080994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results:\n",
        "\n",
        "The MLP trains fairly quickly and has resulted in a surprisingly high accuracy rate, even though it doesn't explicitly exploit any of the underlying network structure. We suspect that this is the case because we have a lot of labeled data (80% of ~1.57M nodes) we can train on. This is also reflected in the fact that the MLP performs very similarly on the Validation and Test nodes as well, indicating that it managed to generalize as well as it architecture allowed it to. To see if we can do better than this, next we will try to use some GNN architectures that will try to embed the structure underlying our data explicitly before training an MLP classifier."
      ],
      "metadata": {
        "id": "o2F4lPUnlOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7y8gotYtYlks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}